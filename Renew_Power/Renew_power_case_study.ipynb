{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renew power case study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1:\n",
    "Approach : \n",
    "Turbine is not connected to server if it satisfies any of the 2 conditions  -\n",
    "\n",
    "•\tCase A : if state = 4 , Or\n",
    "\n",
    "•\tCase B : if state=0 and wind speed <=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.40.4.56:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0.cloudera4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark2-py3-notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=spark2-py3-notebook>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialize SparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant libraries\n",
    "from pyspark.sql import functions as psf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read file into a Spark Dataframe\n",
    "df = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(\"hdfs:/user/kushalu/renew_power_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+--------------------------+-----------------+\n",
      "|TimeStamp (India Standard Time UTC+05:30)|LAHT461-CommunicationState|LAHT461-WindSpeed|\n",
      "+-----------------------------------------+--------------------------+-----------------+\n",
      "|01/09/2019 00:00:07                      |null                      |3.541333         |\n",
      "|01/09/2019 00:00:17                      |null                      |3.381333         |\n",
      "|01/09/2019 00:00:37                      |null                      |3.621333         |\n",
      "|01/09/2019 00:00:47                      |null                      |3.498667         |\n",
      "|01/09/2019 00:00:57                      |null                      |3.397333         |\n",
      "+-----------------------------------------+--------------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#display 5 rows\n",
    "df.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data manipulation to rename certain columns, and set non missing to -1 in case of speed and state in order to prevent \n",
    "# issue when doing comparison filtering on these columns\n",
    "df1=df.withColumnRenamed('TimeStamp (India Standard Time UTC+05:30)','date_id')\\\n",
    ".withColumn('state',\n",
    "            psf.when(\n",
    "                psf.col('LAHT461-CommunicationState').isNotNull(),\n",
    "                psf.col('LAHT461-CommunicationState')\n",
    "            ).otherwise(-1)\n",
    "           )\\\n",
    ".withColumn('speed',\n",
    "            psf.when(\n",
    "                psf.col('LAHT461-WindSpeed').isNotNull(),\n",
    "                psf.col('LAHT461-WindSpeed')\n",
    "            ).otherwise(-1)\n",
    "           )\\\n",
    ".drop('LAHT461-CommunicationState').drop('LAHT461-WindSpeed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+-----+\n",
      "|date_id            |speed   |state|\n",
      "+-------------------+--------+-----+\n",
      "|01/09/2019 00:00:07|3.541333|-1   |\n",
      "|01/09/2019 00:00:17|3.381333|-1   |\n",
      "|01/09/2019 00:00:37|3.621333|-1   |\n",
      "|01/09/2019 00:00:47|3.498667|-1   |\n",
      "|01/09/2019 00:00:57|3.397333|-1   |\n",
      "+-------------------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+-------------------+\n",
      "|sum(not_connected)|sum(row_dummy)|  pct_not_connected|\n",
      "+------------------+--------------+-------------------+\n",
      "|                25|         43139|0.05795220102459491|\n",
      "+------------------+--------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#method 1 : using Spark DataFrame\n",
    "'''\n",
    "Step 1: derive column where - \n",
    "* turbine is not connected. Its value is 1 if not connected, else 0 : column name = 'not_connected'\n",
    "* set a dummy value to 1 for row count : column name = 'row_dummy'\n",
    "\n",
    "Step 2: find sum of columns mentioned above\n",
    "\n",
    "Step 3: find percentage not connected in column 'pct_not_connected' using -\n",
    "sum(not_connected)/(total number of rows)\n",
    "\n",
    "'''\n",
    "df2 = df1.withColumn('not_connected', \n",
    "                     psf.when( (psf.col('state')==4) | \n",
    "                               ( (psf.col('speed')<=0) & (psf.col('state')==0)   )  , 1).otherwise(0) )\\\n",
    ".withColumn('row_dummy', psf.lit(1) ).agg( {'not_connected':'sum' , 'row_dummy':'sum' }  )\\\n",
    ".withColumn('pct_not_connected',psf.col('sum(not_connected)')*100/psf.col('sum(row_dummy)') )\\\n",
    ".show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method 2: Using Spark SQL\n",
    "df1.registerTempTable('renew_power')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|  pct_not_connected|\n",
      "+-------------------+\n",
      "|0.05795220102459491|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('''\n",
    "select (count(date_id)*100/ first(record_count)) as pct_not_connected from \n",
    "(select \n",
    "date_id,\n",
    "speed,\n",
    "lag(state,1,-1) over (order by date_id) as prev_state1,\n",
    "state,\n",
    "lead(state,1,-1) over (order by date_id) as next_state1,\n",
    "row_number() over (order by date_id) as row_number,\n",
    "count(date_id) over (order by date_id rows between unbounded preceding and unbounded following) as record_count\n",
    "from renew_power )\n",
    "where state = 4 or (state=0 and speed<=0)\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Que 2: reading Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name='renew_sample_input_test.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_name) as json_file:\n",
    "    json_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['objects', 'indicators', 'data', 'self'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_date=json_data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField,StructType\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_ = StructType( [    \n",
    "               StructField(\"timestamp\", StringType(), True),\n",
    "              StructField(\"power.ac\", DoubleType(), True),\n",
    "               StructField(\"voltage.ac.ab\", DoubleType(), True),\n",
    "               StructField(\"current.ac\", DoubleType(), True)\n",
    "            ]           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datarows = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-20T09:00:00 890083.7333333333 604.7386666666666 857.0843777777778\n",
      "2020-01-20T09:15:00 985881.5333333333 604.9938000000001 949.5994222222222\n",
      "2020-01-20T09:30:00 1079118.9333333333 605.0138000000001 1039.5389333333333\n",
      "2020-01-20T09:45:00 1173847.3333333333 604.0386 1130.607177777778\n",
      "2020-01-20T10:00:00 1260267.6666666667 606.0834 1211.7521111111112\n",
      "2020-01-20T10:15:00 1335955.6666666667 604.6751999999999 1286.6725777777776\n",
      "2020-01-20T10:30:00 1393353.2 604.5897333333335 1342.1388\n",
      "2020-01-20T10:45:00 1412581.5333333334 606.2603333333333 1354.6253777777777\n",
      "2020-01-20T11:00:00 1483242.3333333333 606.7202 1422.1712888888887\n",
      "2020-01-20T11:15:00 1407966.8 608.2414666666666 1348.0626222222222\n",
      "2020-01-20T11:30:00 1446273.5333333334 612.4508000000001 1372.674088888889\n",
      "2020-01-20T11:45:00 1103368.9333333333 615.1485333333333 1041.5581777777777\n"
     ]
    }
   ],
   "source": [
    "for i in base_date:\n",
    "    print (i[0], i[1][0][0],i[1][0][1],i[1][0][2])\n",
    "    tuple_input = (i[0], i[1][0][0],i[1][0][1],i[1][0][2])\n",
    "    datarows.append(tuple_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_df = sqlContext.createDataFrame(datarows,schema_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-----------------+------------------+\n",
      "|          timestamp|          power.ac|    voltage.ac.ab|        current.ac|\n",
      "+-------------------+------------------+-----------------+------------------+\n",
      "|2020-01-20T09:00:00| 890083.7333333333|604.7386666666666| 857.0843777777778|\n",
      "|2020-01-20T09:15:00| 985881.5333333333|604.9938000000001| 949.5994222222222|\n",
      "|2020-01-20T09:30:00|1079118.9333333333|605.0138000000001|1039.5389333333333|\n",
      "|2020-01-20T09:45:00|1173847.3333333333|         604.0386| 1130.607177777778|\n",
      "|2020-01-20T10:00:00|1260267.6666666667|         606.0834|1211.7521111111112|\n",
      "|2020-01-20T10:15:00|1335955.6666666667|604.6751999999999|1286.6725777777776|\n",
      "|2020-01-20T10:30:00|         1393353.2|604.5897333333335|         1342.1388|\n",
      "|2020-01-20T10:45:00|1412581.5333333334|606.2603333333333|1354.6253777777777|\n",
      "|2020-01-20T11:00:00|1483242.3333333333|         606.7202|1422.1712888888887|\n",
      "|2020-01-20T11:15:00|         1407966.8|608.2414666666666|1348.0626222222222|\n",
      "|2020-01-20T11:30:00|1446273.5333333334|612.4508000000001| 1372.674088888889|\n",
      "|2020-01-20T11:45:00|1103368.9333333333|615.1485333333333|1041.5581777777777|\n",
      "+-------------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "solar_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save results to a csv file\n",
    "solar_df.coalesce(1).write\\\n",
    "        .format(\"com.databricks.spark.csv\")\\\n",
    "        .option(\"header\", \"True\")\\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .save('solar_df_output')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CRV CN DM TEST(Spark 2.2.0 - Python 3.6)",
   "language": "python",
   "name": "crv_cn_dm_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
